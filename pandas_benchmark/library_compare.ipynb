{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc660916",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-09 18:02:44,116\tINFO services.py:1456 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RayContext(dashboard_url='127.0.0.1:8265', python_version='3.9.13', ray_version='1.12.1', ray_commit='4863e33856b54ccf8add5cbe75e41558850a1b75', address_info={'node_ip_address': '127.0.0.1', 'raylet_ip_address': '127.0.0.1', 'redis_address': None, 'object_store_address': 'tcp://127.0.0.1:64965', 'raylet_socket_name': 'tcp://127.0.0.1:64867', 'webui_url': '127.0.0.1:8265', 'session_dir': 'C:\\\\Users\\\\jay\\\\AppData\\\\Local\\\\Temp\\\\ray\\\\session_2023-03-09_18-02-42_164448_18244', 'metrics_export_port': 65435, 'gcs_address': '127.0.0.1:64835', 'address': '127.0.0.1:64835', 'node_id': '2e018927715cd6c95a800a3b73cd308aa46552201c3842ce72f219dc'})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "#import pandas as pd\n",
    "import modin.pandas as mpd\n",
    "import ray\n",
    "ignore_reinit_error=True\n",
    "ray.init()\n",
    "#import polars as pl\n",
    "#from datatable import dt, f, by, g, join, sort, update, ifelse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "174b89d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_1 = '10_million_1.csv'\n",
    "file_2 = '10_million_2.csv'\n",
    "num_ran = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ace5dc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lib_name = 'pandas'\n",
    "stats = []\n",
    "\n",
    "for i in range (num_ran):\n",
    "    stats_inner = {}\n",
    "    start = time.time()\n",
    "    df_1 = pd.read_csv(file_1,  engine=\"pyarrow\", use_nullable_dtypes=True, index_col = 0)\n",
    "    print(f'loading csv with {lib_name} took: {time.time() - start} seconds')\n",
    "    stats_inner['loading_1'] = (time.time() - start)\n",
    "    \n",
    "    start = time.time()\n",
    "    df_2 = pd.read_csv(file_2, engine=\"pyarrow\", use_nullable_dtypes=True, index_col = 0)\n",
    "    print(f'loading csv with {lib_name} took: {time.time() - start} seconds')\n",
    "    stats_inner['loading_2'] = (time.time() - start)\n",
    "    \n",
    "    start = time.time()\n",
    "    df_1.merge(df_2, on = 'unique_id_text' )\n",
    "    print(f'merge csv on unique_id_text with {lib_name} took: {time.time() - start} seconds')\n",
    "    stats_inner['merging'] = (time.time() - start)\n",
    "\n",
    "    ## out of memory\n",
    "    start=time.time()\n",
    "    pd.concat([df_1, df_2])\n",
    "    print(f'concat csv with {lib_name} took: {time.time() - start} seconds')\n",
    "    stats_inner['concat'] = (time.time() - start)\n",
    "    \n",
    "    start=time.time()\n",
    "    df_1.groupby('text_id').sum()\n",
    "    print(f'groupby and sum by text_id with {lib_name} took: {time.time() - start} seconds ')\n",
    "    stats_inner['groupby'] = (time.time() - start)\n",
    "    \n",
    "    start=time.time()\n",
    "    df_1['new_col'] = df_1['0'].apply(round)\n",
    "    print(f'apply round method with {lib_name} took {time.time() - start} seconds.')\n",
    "    stats_inner['apply'] = (time.time() - start)\n",
    "    \n",
    "    start=time.time()\n",
    "    df_1.loc[df_1['text_id'] == '9999_ID']\n",
    "    print(f'.loc filter took {time.time() - start} seconds.')\n",
    "    stats_inner['filtering'] = (time.time() - start)\n",
    "    \n",
    "    start=time.time()\n",
    "    df_1.loc[df_1['text_id'] == '9999_ID', 'text_id'] = 'found it'\n",
    "    print(f'.loc filter and update value took {time.time() - start} seconds.')\n",
    "    stats_inner['filtering & updating'] = (time.time() - start)\n",
    "    \n",
    "    start=time.time()\n",
    "    df_1['0_new'] = df_1['0'] * 2 + 1\n",
    "    print(f'simple calculation took {time.time() - start} seconds.')\n",
    "    stats_inner['column calculation'] = (time.time() - start)\n",
    "    \n",
    "    stats.append(stats_inner)\n",
    "    \n",
    "print([k for k in sorted(stats[0].keys())])\n",
    "pandas_stats = [round((stats[0][k] + stats[1][k])/2,5) for k in sorted(stats[0].keys())]\n",
    "print(pandas_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5e5c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv(file_1,  engine=\"pyarrow\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f6d7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98866c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1['unique_id_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69129e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a442557",
   "metadata": {},
   "source": [
    "# modin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "331aa5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading csv with modin took: 7.027432203292847 seconds\n",
      "loading csv with modin took: 5.830769777297974 seconds\n",
      "concat csv with modin took: 0.04370999336242676 seconds\n",
      "groupby and sum by text_id with modin took: 3.1504709720611572 seconds \n",
      "apply round method with modin took 0.9100008010864258 seconds.\n",
      ".loc filter took 0.6415708065032959 seconds.\n",
      ".loc filter and update value took 1.4270000457763672 seconds.\n",
      "simple calculation took 0.8335258960723877 seconds.\n",
      "loading csv with modin took: 4.644488334655762 seconds\n",
      "loading csv with modin took: 4.509963512420654 seconds\n",
      "concat csv with modin took: 0.04200029373168945 seconds\n",
      "groupby and sum by text_id with modin took: 2.8525197505950928 seconds \n",
      "apply round method with modin took 0.8770017623901367 seconds.\n",
      ".loc filter took 0.9575190544128418 seconds.\n",
      ".loc filter and update value took 1.3809998035430908 seconds.\n",
      "simple calculation took 0.8525257110595703 seconds.\n",
      "['apply', 'column calculation', 'concat', 'filtering', 'filtering & updating', 'groupby', 'loading_1', 'loading_2']\n",
      "[0.8935, 0.84303, 0.04286, 0.79954, 1.404, 3.0015, 5.83646, 5.17037]\n"
     ]
    }
   ],
   "source": [
    "lib_name = 'modin'\n",
    "\n",
    "stats = []\n",
    "\n",
    "for i in range (num_ran):\n",
    "    stats_inner = {}\n",
    "    start = time.time()\n",
    "    df_1 = mpd.read_csv(file_1,  index_col = 0)\n",
    "    print(f'loading csv with {lib_name} took: {time.time() - start} seconds')\n",
    "    stats_inner['loading_1'] = (time.time() - start)\n",
    "    \n",
    "    start = time.time()\n",
    "    df_2 = mpd.read_csv(file_2,  index_col = 0)\n",
    "    print(f'loading csv with {lib_name} took: {time.time() - start} seconds')\n",
    "    stats_inner['loading_2'] = (time.time() - start)\n",
    "    \n",
    "#     start = time.time()\n",
    "#     df_1.merge(df_2, on = 'unique_id_text' )\n",
    "#     print(f'merge csv on unique_id_text with {lib_name} took: {time.time() - start} seconds')\n",
    "#     stats_inner['merging'] = (time.time() - start)\n",
    "\n",
    "    ## out of memory\n",
    "    start=time.time()\n",
    "    mpd.concat([df_1, df_2])\n",
    "    print(f'concat csv with {lib_name} took: {time.time() - start} seconds')\n",
    "    stats_inner['concat'] = (time.time() - start)\n",
    "    \n",
    "    start=time.time()\n",
    "    df_1.groupby('text_id').sum()\n",
    "    print(f'groupby and sum by text_id with {lib_name} took: {time.time() - start} seconds ')\n",
    "    stats_inner['groupby'] = (time.time() - start)\n",
    "    \n",
    "    start=time.time()\n",
    "    df_1['new_col'] = df_1['0'].apply(round)\n",
    "    print(f'apply round method with {lib_name} took {time.time() - start} seconds.')\n",
    "    stats_inner['apply'] = (time.time() - start)\n",
    "    \n",
    "    start=time.time()\n",
    "    df_1.loc[df_1['text_id'] == '9999_ID']\n",
    "    print(f'.loc filter took {time.time() - start} seconds.')\n",
    "    stats_inner['filtering'] = (time.time() - start)\n",
    "    \n",
    "    start=time.time()\n",
    "    df_1.loc[df_1['text_id'] == '9999_ID', 'text_id'] = 'found it'\n",
    "    print(f'.loc filter and update value took {time.time() - start} seconds.')\n",
    "    stats_inner['filtering & updating'] = (time.time() - start)\n",
    "    \n",
    "    start=time.time()\n",
    "    df_1['0_new'] = df_1['0'] * 2 + 1\n",
    "    print(f'simple calculation took {time.time() - start} seconds.')\n",
    "    stats_inner['column calculation'] = (time.time() - start)\n",
    "    \n",
    "    stats.append(stats_inner)\n",
    "    \n",
    "print([k for k in sorted(stats[0].keys())])\n",
    "modin_stats = [round((stats[0][k] + stats[1][k])/2,5) for k in sorted(stats[0].keys())]\n",
    "print(modin_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2aa00f",
   "metadata": {},
   "source": [
    "# polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251c0cb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lib_name = 'polars'\n",
    "stats = []\n",
    "\n",
    "for i in range(num_ran):\n",
    "    stats_inner = {}\n",
    "    start = time.time()\n",
    "    df_1 = pl.read_csv(file_1)\n",
    "    print(f'loading csv with {lib_name} took: {time.time()-start} seconds.')\n",
    "    stats_inner['loading_1'] = (time.time() - start)\n",
    "\n",
    "    start = time.time()\n",
    "    df_2 = pl.read_csv(file_2)\n",
    "    print(f'loading csv with {lib_name} took: {time.time()-start} seconds.')\n",
    "    stats_inner['loading_2'] = (time.time() - start)\n",
    "\n",
    "    start=time.time()\n",
    "    pl.concat([df_1, df_2])\n",
    "    print(f'concat (append) csv with {lib_name} took: {time.time() - start} seconds')\n",
    "    stats_inner['concat'] = (time.time() - start)\n",
    "\n",
    "    start=time.time()\n",
    "    df_1.join(df_2, on = 'unique_id_text')\n",
    "    print(f'join (merge) csv with {lib_name} took: {time.time() - start} seconds')\n",
    "    stats_inner['merging'] = (time.time() - start)\n",
    "\n",
    "    start=time.time()\n",
    "    df_1.groupby('text_id').sum()\n",
    "    print(f'groupby and sum by text_id with {lib_name} took: {time.time() - start} seconds ')\n",
    "    stats_inner['groupby'] = (time.time() - start)\n",
    "\n",
    "    start=time.time()\n",
    "    df_1['new_col'] = df_1['0'].apply(round)\n",
    "    print(f'apply round method with {lib_name} took {time.time() - start} seconds.')\n",
    "    stats_inner['apply'] = (time.time() - start)\n",
    "\n",
    "    start=time.time()\n",
    "    df_1[df_1['text_id'] == '9999_ID']\n",
    "    print(f'filter took {time.time() - start} seconds.')\n",
    "    stats_inner['filtering'] = (time.time() - start)\n",
    "\n",
    "    start=time.time()\n",
    "    df_1[df_1['text_id'] == '9999_ID', 'text_id'] = 'found it'\n",
    "    print(f'filter and update value took {time.time() - start} seconds.')\n",
    "    stats_inner['filtering & updating'] = (time.time() - start)\n",
    "\n",
    "    start=time.time()\n",
    "    df_1['0_2'] = df_1['0'] *2 + 1\n",
    "    print(f'simple column calculation with {lib_name} took {time.time() - start} seconds.')\n",
    "    stats_inner['column calculation'] = (time.time() - start)\n",
    "    \n",
    "    stats.append(stats_inner)\n",
    "\n",
    "print([k for k in sorted(stats[0].keys())])\n",
    "polars_stats = [round((stats[0][k] + stats[1][k])/2,5) for k in sorted(stats[0].keys())]\n",
    "print(polars_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6addf795",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pl.read_csv(file_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8c302d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6504a280",
   "metadata": {},
   "source": [
    "# datatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c5203c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lib_name = 'datatable'\n",
    "stats = []\n",
    "\n",
    "for i in range(num_ran):\n",
    "    stats_inner = {}\n",
    "    start = time.time()\n",
    "    df_1 = dt.fread(file_1)\n",
    "    print(f'loading csv with {lib_name} took: {time.time()-start} seconds.')\n",
    "    stats_inner['loading_1'] = (time.time() - start)\n",
    "    \n",
    "    start = time.time()\n",
    "    df_2 = dt.fread(file_2)\n",
    "    print(f'loading csv with {lib_name} took: {time.time()-start} seconds.')\n",
    "    stats_inner['loading_2'] = (time.time() - start)\n",
    "\n",
    "    start = time.time()\n",
    "    df_2 = dt.rbind(df_1, df_2)\n",
    "    print(f'vertically concat csv with {lib_name} took: {time.time()-start} seconds.')\n",
    "    stats_inner['concat'] = (time.time() - start)\n",
    "    \n",
    "    df_3 = df_1[:,0:10]\n",
    "    start = time.time()\n",
    "    df_3[:,dt.sum(f[:]), by('text_id')]\n",
    "    print(f'groupby and sum csv with {lib_name} took: {time.time()-start} seconds.')\n",
    "    stats_inner['groupby'] = (time.time() - start)\n",
    "    \n",
    "    start = time.time()\n",
    "    df_1['0_2'] = f[1] * 2 +1\n",
    "    print(f'simple column calculation with {lib_name} took: {time.time()-start} seconds.')\n",
    "    stats_inner['column calculation'] = (time.time() - start)\n",
    "    \n",
    "    start = time.time()\n",
    "    df_1[f.text_id == '9999_ID',:]\n",
    "    print(f'filtering with {lib_name} took: {time.time()-start} seconds.')\n",
    "    stats_inner['filtering'] = (time.time() - start)\n",
    "    \n",
    "    start = time.time()\n",
    "    df_1[f.text_id == '9999_ID','text_id'] = 'found it'\n",
    "    print(f'filtering and update value with {lib_name} took: {time.time()-start} seconds.')\n",
    "    stats_inner['filtering & updating'] = (time.time() - start)\n",
    "    \n",
    "    stats_inner['merge'] = 0\n",
    "    stats.append(stats_inner)\n",
    "\n",
    "print([k for k in sorted(stats[0].keys())])\n",
    "datatable_stats = [round((stats[0][k] + stats[1][k])/2,5) for k in sorted(stats[0].keys())]\n",
    "print(datatable_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506853eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a24d471",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b640de47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time test('pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96594053",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time test('modin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeefb624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bc89b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pl.read_csv(file_1)\n",
    "df2 = pl.read_csv(file_2)\n",
    "df3 = pl.concat([df1, df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346ec2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f737267",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df1.join(df2, on ='unique_id_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a810e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c1f5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc8644d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.groupby('text_id').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73354d2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
