{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc660916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "#import modin.pandas as mpd\n",
    "#import ray\n",
    "#ignore_reinit_error=True\n",
    "#ray.init()\n",
    "#import polars as pl\n",
    "#from datatable import dt, f, by, g, join, sort, update, ifelse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "174b89d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_1 = '10_million_1.csv'\n",
    "file_2 = '10_million_2.csv'\n",
    "num_ran = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ace5dc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "read_csv() got an unexpected keyword argument 'use_nullable_dtypes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m stats_inner \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      6\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 7\u001b[0m df_1 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpyarrow\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloading csv with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlib_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m took: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m stats_inner[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloading_1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: read_csv() got an unexpected keyword argument 'use_nullable_dtypes'"
     ]
    }
   ],
   "source": [
    "lib_name = 'pandas'\n",
    "stats = []\n",
    "\n",
    "for i in range (num_ran):\n",
    "    stats_inner = {}\n",
    "    start = time.time()\n",
    "    df_1 = pd.read_csv(file_1,  engine=\"pyarrow\", use_nullable_dtypes=True, index_col = 0)\n",
    "    print(f'loading csv with {lib_name} took: {time.time() - start} seconds')\n",
    "    stats_inner['loading_1'] = (time.time() - start)\n",
    "    \n",
    "    start = time.time()\n",
    "    df_2 = pd.read_csv(file_2, engine=\"pyarrow\", use_nullable_dtypes=True, index_col = 0)\n",
    "    print(f'loading csv with {lib_name} took: {time.time() - start} seconds')\n",
    "    stats_inner['loading_2'] = (time.time() - start)\n",
    "    \n",
    "    start = time.time()\n",
    "    df_1.merge(df_2, on = 'unique_id_text' )\n",
    "    print(f'merge csv on unique_id_text with {lib_name} took: {time.time() - start} seconds')\n",
    "    stats_inner['merging'] = (time.time() - start)\n",
    "\n",
    "    ## out of memory\n",
    "    start=time.time()\n",
    "    pd.concat([df_1, df_2])\n",
    "    print(f'concat csv with {lib_name} took: {time.time() - start} seconds')\n",
    "    stats_inner['concat'] = (time.time() - start)\n",
    "    \n",
    "    start=time.time()\n",
    "    df_1.groupby('text_id').sum()\n",
    "    print(f'groupby and sum by text_id with {lib_name} took: {time.time() - start} seconds ')\n",
    "    stats_inner['groupby'] = (time.time() - start)\n",
    "    \n",
    "    start=time.time()\n",
    "    df_1['new_col'] = df_1['0'].apply(round)\n",
    "    print(f'apply round method with {lib_name} took {time.time() - start} seconds.')\n",
    "    stats_inner['apply'] = (time.time() - start)\n",
    "    \n",
    "    start=time.time()\n",
    "    df_1.loc[df_1['text_id'] == '9999_ID']\n",
    "    print(f'.loc filter took {time.time() - start} seconds.')\n",
    "    stats_inner['filtering'] = (time.time() - start)\n",
    "    \n",
    "    start=time.time()\n",
    "    df_1.loc[df_1['text_id'] == '9999_ID', 'text_id'] = 'found it'\n",
    "    print(f'.loc filter and update value took {time.time() - start} seconds.')\n",
    "    stats_inner['filtering & updating'] = (time.time() - start)\n",
    "    \n",
    "    start=time.time()\n",
    "    df_1['0_new'] = df_1['0'] * 2 + 1\n",
    "    print(f'simple calculation took {time.time() - start} seconds.')\n",
    "    stats_inner['column calculation'] = (time.time() - start)\n",
    "    \n",
    "    stats.append(stats_inner)\n",
    "    \n",
    "print([k for k in sorted(stats[0].keys())])\n",
    "pandas_stats = [round((stats[0][k] + stats[1][k])/2,5) for k in sorted(stats[0].keys())]\n",
    "print(pandas_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f5e5c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv(file_1,  engine=\"pyarrow\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5f6d7a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>integer_id</th>\n",
       "      <th>text_id</th>\n",
       "      <th>unique_id_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8853495</th>\n",
       "      <td>0.855285</td>\n",
       "      <td>0.240642</td>\n",
       "      <td>0.732951</td>\n",
       "      <td>0.860857</td>\n",
       "      <td>0.956267</td>\n",
       "      <td>0.416427</td>\n",
       "      <td>0.050623</td>\n",
       "      <td>4780</td>\n",
       "      <td>4780_ID</td>\n",
       "      <td>8853495_ID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4844597</th>\n",
       "      <td>0.901882</td>\n",
       "      <td>0.999457</td>\n",
       "      <td>0.978952</td>\n",
       "      <td>0.559693</td>\n",
       "      <td>0.859066</td>\n",
       "      <td>0.737484</td>\n",
       "      <td>0.713445</td>\n",
       "      <td>8340</td>\n",
       "      <td>8340_ID</td>\n",
       "      <td>4844597_ID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3912746</th>\n",
       "      <td>0.350191</td>\n",
       "      <td>0.766841</td>\n",
       "      <td>0.572385</td>\n",
       "      <td>0.496385</td>\n",
       "      <td>0.503123</td>\n",
       "      <td>0.646241</td>\n",
       "      <td>0.951089</td>\n",
       "      <td>4882</td>\n",
       "      <td>4882_ID</td>\n",
       "      <td>3912746_ID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3143505</th>\n",
       "      <td>0.311571</td>\n",
       "      <td>0.042855</td>\n",
       "      <td>0.774996</td>\n",
       "      <td>0.232767</td>\n",
       "      <td>0.301296</td>\n",
       "      <td>0.576442</td>\n",
       "      <td>0.217577</td>\n",
       "      <td>9588</td>\n",
       "      <td>9588_ID</td>\n",
       "      <td>3143505_ID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030606</th>\n",
       "      <td>0.848233</td>\n",
       "      <td>0.081258</td>\n",
       "      <td>0.754872</td>\n",
       "      <td>0.809918</td>\n",
       "      <td>0.961630</td>\n",
       "      <td>0.065941</td>\n",
       "      <td>0.284760</td>\n",
       "      <td>8066</td>\n",
       "      <td>8066_ID</td>\n",
       "      <td>1030606_ID</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1         2         3         4         5         6  \\\n",
       "                                                                                \n",
       "8853495  0.855285  0.240642  0.732951  0.860857  0.956267  0.416427  0.050623   \n",
       "4844597  0.901882  0.999457  0.978952  0.559693  0.859066  0.737484  0.713445   \n",
       "3912746  0.350191  0.766841  0.572385  0.496385  0.503123  0.646241  0.951089   \n",
       "3143505  0.311571  0.042855  0.774996  0.232767  0.301296  0.576442  0.217577   \n",
       "1030606  0.848233  0.081258  0.754872  0.809918  0.961630  0.065941  0.284760   \n",
       "\n",
       "         integer_id  text_id unique_id_text  \n",
       "                                             \n",
       "8853495        4780  4780_ID     8853495_ID  \n",
       "4844597        8340  8340_ID     4844597_ID  \n",
       "3912746        4882  4882_ID     3912746_ID  \n",
       "3143505        9588  9588_ID     3143505_ID  \n",
       "1030606        8066  8066_ID     1030606_ID  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98866c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "8853495    8853495_ID\n",
       "4844597    4844597_ID\n",
       "3912746    3912746_ID\n",
       "3143505    3143505_ID\n",
       "1030606    1030606_ID\n",
       "              ...    \n",
       "402204      402204_ID\n",
       "5829411    5829411_ID\n",
       "1434403    1434403_ID\n",
       "6754697    6754697_ID\n",
       "5921685    5921685_ID\n",
       "Name: unique_id_text, Length: 10000000, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1['unique_id_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69129e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a442557",
   "metadata": {},
   "source": [
    "# modin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "331aa5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading csv with modin took: 1.4963836669921875 seconds\n",
      "loading csv with modin took: 0.46642446517944336 seconds\n",
      "concat csv with modin took: 0.005003929138183594 seconds\n",
      "groupby and sum by text_id with modin took: 1.2561426162719727 seconds \n",
      "apply round method with modin took 0.08707928657531738 seconds.\n",
      ".loc filter took 0.09308457374572754 seconds.\n",
      ".loc filter and update value took 0.08808016777038574 seconds.\n",
      "simple calculation took 0.07406759262084961 seconds.\n",
      "loading csv with modin took: 0.7657451629638672 seconds\n",
      "loading csv with modin took: 0.38288092613220215 seconds\n",
      "concat csv with modin took: 0.0030024051666259766 seconds\n",
      "groupby and sum by text_id with modin took: 1.5353965759277344 seconds \n",
      "apply round method with modin took 0.08707928657531738 seconds.\n",
      ".loc filter took 0.09108328819274902 seconds.\n",
      ".loc filter and update value took 0.09008121490478516 seconds.\n",
      "simple calculation took 0.3478207588195801 seconds.\n",
      "['apply', 'column calculation', 'concat', 'filtering', 'filtering & updating', 'groupby', 'loading_1', 'loading_2']\n",
      "[0.08708, 0.21094, 0.004, 0.09208, 0.08908, 1.39577, 1.13156, 0.42465]\n"
     ]
    }
   ],
   "source": [
    "lib_name = 'modin'\n",
    "\n",
    "stats = []\n",
    "\n",
    "for i in range (num_ran):\n",
    "    stats_inner = {}\n",
    "    start = time.time()\n",
    "    df_1 = mpd.read_csv(file_1,  index_col = 0)\n",
    "    print(f'loading csv with {lib_name} took: {time.time() - start} seconds')\n",
    "    stats_inner['loading_1'] = (time.time() - start)\n",
    "    \n",
    "    start = time.time()\n",
    "    df_2 = mpd.read_csv(file_2,  index_col = 0)\n",
    "    print(f'loading csv with {lib_name} took: {time.time() - start} seconds')\n",
    "    stats_inner['loading_2'] = (time.time() - start)\n",
    "    \n",
    "#     start = time.time()\n",
    "#     df_1.merge(df_2, on = 'unique_id_text' )\n",
    "#     print(f'merge csv on unique_id_text with {lib_name} took: {time.time() - start} seconds')\n",
    "#     stats_inner['merging'] = (time.time() - start)\n",
    "\n",
    "    ## out of memory\n",
    "    start=time.time()\n",
    "    mpd.concat([df_1, df_2])\n",
    "    print(f'concat csv with {lib_name} took: {time.time() - start} seconds')\n",
    "    stats_inner['concat'] = (time.time() - start)\n",
    "    \n",
    "    start=time.time()\n",
    "    df_1.groupby('text_id').sum()\n",
    "    print(f'groupby and sum by text_id with {lib_name} took: {time.time() - start} seconds ')\n",
    "    stats_inner['groupby'] = (time.time() - start)\n",
    "    \n",
    "    start=time.time()\n",
    "    df_1['new_col'] = df_1['0'].apply(round)\n",
    "    print(f'apply round method with {lib_name} took {time.time() - start} seconds.')\n",
    "    stats_inner['apply'] = (time.time() - start)\n",
    "    \n",
    "    start=time.time()\n",
    "    df_1.loc[df_1['text_id'] == '9999_ID']\n",
    "    print(f'.loc filter took {time.time() - start} seconds.')\n",
    "    stats_inner['filtering'] = (time.time() - start)\n",
    "    \n",
    "    start=time.time()\n",
    "    df_1.loc[df_1['text_id'] == '9999_ID', 'text_id'] = 'found it'\n",
    "    print(f'.loc filter and update value took {time.time() - start} seconds.')\n",
    "    stats_inner['filtering & updating'] = (time.time() - start)\n",
    "    \n",
    "    start=time.time()\n",
    "    df_1['0_new'] = df_1['0'] * 2 + 1\n",
    "    print(f'simple calculation took {time.time() - start} seconds.')\n",
    "    stats_inner['column calculation'] = (time.time() - start)\n",
    "    \n",
    "    stats.append(stats_inner)\n",
    "    \n",
    "print([k for k in sorted(stats[0].keys())])\n",
    "modin_stats = [round((stats[0][k] + stats[1][k])/2,5) for k in sorted(stats[0].keys())]\n",
    "print(modin_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2aa00f",
   "metadata": {},
   "source": [
    "# polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251c0cb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lib_name = 'polars'\n",
    "stats = []\n",
    "\n",
    "for i in range(num_ran):\n",
    "    stats_inner = {}\n",
    "    start = time.time()\n",
    "    df_1 = pl.read_csv(file_1)\n",
    "    print(f'loading csv with {lib_name} took: {time.time()-start} seconds.')\n",
    "    stats_inner['loading_1'] = (time.time() - start)\n",
    "\n",
    "    start = time.time()\n",
    "    df_2 = pl.read_csv(file_2)\n",
    "    print(f'loading csv with {lib_name} took: {time.time()-start} seconds.')\n",
    "    stats_inner['loading_2'] = (time.time() - start)\n",
    "\n",
    "    start=time.time()\n",
    "    pl.concat([df_1, df_2])\n",
    "    print(f'concat (append) csv with {lib_name} took: {time.time() - start} seconds')\n",
    "    stats_inner['concat'] = (time.time() - start)\n",
    "\n",
    "    start=time.time()\n",
    "    df_1.join(df_2, on = 'unique_id_text')\n",
    "    print(f'join (merge) csv with {lib_name} took: {time.time() - start} seconds')\n",
    "    stats_inner['merging'] = (time.time() - start)\n",
    "\n",
    "    start=time.time()\n",
    "    df_1.groupby('text_id').sum()\n",
    "    print(f'groupby and sum by text_id with {lib_name} took: {time.time() - start} seconds ')\n",
    "    stats_inner['groupby'] = (time.time() - start)\n",
    "\n",
    "    start=time.time()\n",
    "    df_1['new_col'] = df_1['0'].apply(round)\n",
    "    print(f'apply round method with {lib_name} took {time.time() - start} seconds.')\n",
    "    stats_inner['apply'] = (time.time() - start)\n",
    "\n",
    "    start=time.time()\n",
    "    df_1[df_1['text_id'] == '9999_ID']\n",
    "    print(f'filter took {time.time() - start} seconds.')\n",
    "    stats_inner['filtering'] = (time.time() - start)\n",
    "\n",
    "    start=time.time()\n",
    "    df_1[df_1['text_id'] == '9999_ID', 'text_id'] = 'found it'\n",
    "    print(f'filter and update value took {time.time() - start} seconds.')\n",
    "    stats_inner['filtering & updating'] = (time.time() - start)\n",
    "\n",
    "    start=time.time()\n",
    "    df_1['0_2'] = df_1['0'] *2 + 1\n",
    "    print(f'simple column calculation with {lib_name} took {time.time() - start} seconds.')\n",
    "    stats_inner['column calculation'] = (time.time() - start)\n",
    "    \n",
    "    stats.append(stats_inner)\n",
    "\n",
    "print([k for k in sorted(stats[0].keys())])\n",
    "polars_stats = [round((stats[0][k] + stats[1][k])/2,5) for k in sorted(stats[0].keys())]\n",
    "print(polars_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6addf795",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pl.read_csv(file_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8c302d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6504a280",
   "metadata": {},
   "source": [
    "# datatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c5203c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lib_name = 'datatable'\n",
    "stats = []\n",
    "\n",
    "for i in range(num_ran):\n",
    "    stats_inner = {}\n",
    "    start = time.time()\n",
    "    df_1 = dt.fread(file_1)\n",
    "    print(f'loading csv with {lib_name} took: {time.time()-start} seconds.')\n",
    "    stats_inner['loading_1'] = (time.time() - start)\n",
    "    \n",
    "    start = time.time()\n",
    "    df_2 = dt.fread(file_2)\n",
    "    print(f'loading csv with {lib_name} took: {time.time()-start} seconds.')\n",
    "    stats_inner['loading_2'] = (time.time() - start)\n",
    "\n",
    "    start = time.time()\n",
    "    df_2 = dt.rbind(df_1, df_2)\n",
    "    print(f'vertically concat csv with {lib_name} took: {time.time()-start} seconds.')\n",
    "    stats_inner['concat'] = (time.time() - start)\n",
    "    \n",
    "    df_3 = df_1[:,0:10]\n",
    "    start = time.time()\n",
    "    df_3[:,dt.sum(f[:]), by('text_id')]\n",
    "    print(f'groupby and sum csv with {lib_name} took: {time.time()-start} seconds.')\n",
    "    stats_inner['groupby'] = (time.time() - start)\n",
    "    \n",
    "    start = time.time()\n",
    "    df_1['0_2'] = f[1] * 2 +1\n",
    "    print(f'simple column calculation with {lib_name} took: {time.time()-start} seconds.')\n",
    "    stats_inner['column calculation'] = (time.time() - start)\n",
    "    \n",
    "    start = time.time()\n",
    "    df_1[f.text_id == '9999_ID',:]\n",
    "    print(f'filtering with {lib_name} took: {time.time()-start} seconds.')\n",
    "    stats_inner['filtering'] = (time.time() - start)\n",
    "    \n",
    "    start = time.time()\n",
    "    df_1[f.text_id == '9999_ID','text_id'] = 'found it'\n",
    "    print(f'filtering and update value with {lib_name} took: {time.time()-start} seconds.')\n",
    "    stats_inner['filtering & updating'] = (time.time() - start)\n",
    "    \n",
    "    stats_inner['merge'] = 0\n",
    "    stats.append(stats_inner)\n",
    "\n",
    "print([k for k in sorted(stats[0].keys())])\n",
    "datatable_stats = [round((stats[0][k] + stats[1][k])/2,5) for k in sorted(stats[0].keys())]\n",
    "print(datatable_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506853eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a24d471",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b640de47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time test('pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96594053",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time test('modin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeefb624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bc89b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pl.read_csv(file_1)\n",
    "df2 = pl.read_csv(file_2)\n",
    "df3 = pl.concat([df1, df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346ec2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f737267",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df1.join(df2, on ='unique_id_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a810e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c1f5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc8644d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.groupby('text_id').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73354d2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
